---
layout: post
title:  "ConvNets"
categories: [design, tools]
tags: [tag1, tag2]
---

## [Convolutional Neural Networks (CNNs / ConvNets)](http://cs231n.github.io/convolutional-networks/)
fenced code block hi `test` hi why not?  
hello, `hello.sidney()` is function!

```python
import numpy as np
def test():
    print "hi"
```

Mathjax test  
$a^2 = b^2 + c^2$  
$$a^2 = b^2 + c^2$$

redcarpet inline mathajx: \\( sin(x^2) \\)

strikethrough  
~~hi~~  
~hi~

[TOC]

CNN은 일반적인 NN과 비슷하다: 뉴런과 학습가능한 weight, bias로 구성되어 있으며 각 뉴런들은 인풋을 받고, 닷 프로덕트를 통해 non-linearity를 구현한다. 전체 네트워크는 여전히 하나의 미분가능한(differentiable) score function으로 표현되고, 여전히 마지막 (fully-connected)레이어에 loss function(SVM/Softmax)를 가지고 있다. 

그럼 뭐가 바뀌었는가? ConvNet 구조는 입력이 이미지라는 명시적인 가정을 하고, 이는 아키텍처에 몇몇 특성들을 심을 수 있게 한다. 이는 forward function을 좀 더 효율적으로 만들고 수많은 파라메터들을 생략할 수 있게 한다.

### Architecture Overview
Recall: Regular Neural Nets. NN은 input(single vector)을 받아서, hidden layer를 통해 변환한다. 각 히든 레이어는 뉴런들로 구성되어 있으며 각 뉴런들은 이전 레이어의 모든 뉴런들과 전부 연결되어 있다(fully-connected). 또한 각 레이어 안에서 뉴런들은 완전히 독립적이고 서로간에 어떠한 커넥션도 공유하지 않는다. 마지막 fully-connected 레이어는 "output layer"라고 불리며 최종적으로 여기에서 class score를 산출한다.

Regular Neural Nets는 full image에 잘 어울리지 않는다. CIFAR-10 데이터셋을 보면, 이미지가 32x32x3(32x32, 3 color channels)로 구성되어 있고 따라서 첫번째 히든 레이어의 하나의 fully-connected neuron은 32*32*3=3072 개의 weight를 가진다. 이 정도는 아직 괜찮아 보이지만, 명백하게 fully-connected 구조는 큰 이미지에 어울리지 않는다(CIFAR-10은 작은 이미지니까 괜찮은 것). 

3D volumes of neurons. CNN은 image가 3D 구조로 되어 있다는 것으로부터 어드밴티지를 얻는다. 즉, regular NN과는 다르게 ConvNet의 뉴런들은 **width, height, depth**의 3차원으로 된 구조를 다룬다. 예를 들어, CIFAR-10의 인풋 이미지는 input volume of activations이고, 이 볼륨은 32x32x3이다. 앞으로 보겠지만, CNN의 뉴런은 fully-connected가 아닌 이전 레이어의 일부 지역과만 연결된다. 더욱이, CIFAR-10의 최종 output layer는 1x1x10인데 이는 ConvNet 구조가 전체 이미지를 하나의 class score 벡터로 축소시키기 때문이다. 다음 그림을 참고하자:

- - -

![Regular NN](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)
![ConvNet](http://cs231n.github.io/assets/cnn/cnn.jpeg)

위는 regular 3-layer NN이고, 아래는 ConvNet이다.

- - -

### Layers used to build ConvNets
위에서 묘사한것처럼, ConvNet의 모든 레이어는 one volume of activation을 미분가능한 함수를 통해 transform한다. ConvNet에서는 주로 세 타입의 레이어를 사용한다: **Convolutional Layer, Pooling Layer** 그리고 **Fully-Connected Layer**. 이 세 레이어를 쌓아서 ConvNet을 만들 것이다.

CIFAR-10 classification을 위한 간단한 ConvNet은 [INPUT - CONV - RELU - POOL - FC]의 구조를 가질 수 있다:  

* INPUT[32x32x3]: 이미지의 픽셀 값을 인풋으로 받음
* CONV layer: 각 뉴런들은 인풋의 local region과 연결되어 있고, 각각의 weight와 인풋의 region의 dot product로 output을 산출한다. 이 레이어의 결과는 [32x32x12]가 된다.
* RELU layer: activation function 각각에 적용되어, **max(0, x)**와 같은 형태로 액티베이션 펑션의 결과가 음수로 나오면 0으로 잘라 버린다. 이 과정은 볼륨의 크기를 바꾸지 않으므로 [32x32x12]가 유지된다.
* POOL layer: spatial dimension(width, height)을 downsampling한다. [16x16x12]가 결과로 나온다.
* FC(fully-connected) layer: [1x1x10]의 벡터로 클래스 스코어를 계산한다. 이 10개의 숫자는 각각 CIFAR-10의 10개의 클래스 카테고리를 의미한다. 이름에서도 알 수 있듯이 이 레이어의 뉴런들은 이전 레이어와 fully-connected된다.

이러한 방법으로, ConvNet은 오리지널 이미지를 레이어들을 통해 오리지널 픽셀에서 최종적인 클래스 스코어로 변환한다. 몇몇 레이어들은 파라메터가 있지만 그렇지 않은 레이어도 있다는 것을 알아두자. 구체적으로, CONV/FC 레이어는 파라메터를 가지고 인풋의 변환을 수행하지만, RELU/POOL 레이어는 고정된 함수에 의해 수행된다. CONV/FC의 파라메터들은 gradient descent에 의해 학습된다.

요약:  

* ConvNet 구조는 이미지 볼륨을 아웃풋 볼륨으로 변환하는 레이어들의 나열이다
* 몇개의 레이어 타입들이 있다 (CONV/FC/RELU/POOL 이 지금까지 가장 유명한 것들)
* 각 레이어는 input 3D volume을 입력받고 이를 미분가능한 함수를 통해 output 3D volume으로 변환한다
* 각 레이어는 파라메터를 가지고 있을 수도 있고 아닐수도 있다 (CONV/FC는 가지고 있고, RELU/POOL은 그렇지 않다)
* 각 레이어는 추가적인 하이퍼파라메터를 가지고 있을 수도 있고 아닐수도 있다 (CONV/FC/POOL은 가지고 있고, RELU는 그렇지 않다)

- - -  
![convnet example](http://cs231n.github.io/assets/cnn/convnet.jpeg)

ConvNet 아키텍처 예제. 초기 인풋 볼륨은 raw image pixels이고, 최종 볼륨은 클래스 스코어다. 각 액티베이션 볼륨은 처리과정을 보여준다. 3D 볼륨을 시각화하기는 어렵기 때문에 2D로 잘라서 보여주고 있다. 이 아키텍처는 작은 VGG Net이고, 아래에서 다룰 것이다.   

- - -   

이제, 각 레이어들의 디테일과 하이퍼파라메터 그리고 커넥션들에 대해 살펴보자.

#### Convolutional Layer
CONV 레이어는 CNN의 핵심 블록이고, 이 레이어의 아웃풋 볼륨은 3D 볼륨의 뉴런들로 볼 수 있다. 우리는 이제 이 뉴런간의 커넥션과, arrangement in space, 그리고 parameter sharing scheme에 대해 살펴볼 것이다.

**Overview and Intuition.** CONV 레이어의 파라메터는 learnable filters들로 구성되어 있다. 모든 필터는 작은 width와 height를 가진 작은 사각형이지만, 인풋 볼륨의 full depth를 통해 확장될 수 있다. 앞으로 진행하는 동안, 우리는 각 필터에 인풋 볼륨을 통과시켜, 그 필터의 2차원 activation map을 만든다. 인풋을 필터에 통과시킨다는 것은, 이 필터와 인풋의 dot product를 계산한다는 의미다. 직관적으로, 이 네트워크는 인풋의 어떤 포지션에서 특정한 타입의 피처를 만나면 활성화되는 필터를 학습한다(즉 특정한 피처를 검출하는 필터를 학습한다). 이 activation map을 output volume만큼 쌓는다. 즉, output volume은 각각 인풋의 일부 작은 region만을 고려하는 뉴런의 아웃풋이다. 또한, 같은 activation map의 뉴런들은 파라메터를 공유한다(같은 필터를 사용한다는 것은 같은 파라메터를 사용한다는 의미인 것 같다).

**Local Connectivity.** 이미지와 같이 고차원의 인풋을 다룰 때는, 위에서 본 것처럼 fully-connected가 비현실적이다. 대신, 인풋 볼륨의 local region만을 각 뉴런에 연결한다. 이 연결의 spatial(width, height) 정도는 뉴런의 **receptive field**라고 불리는 하이퍼파라메터다. depth에 따른 연결 정도는 인풋 볼륨의 depth와 동일하게 항상 일정하다. 이 비대칭의 정도는 우리가 spatial dimension과 depth dimension을 다룰 때 중요하다: 커넥션은 width와 height에 따라 지역적이지만, depth는 항상 인풋 볼륨의 전체 depth를 사용한다. (일부 width, height에 해당하는 local region을 다루지만 depth는 항상 전체를 다 쓴다는 의미인 듯)

*예제 1.* 인풋 볼륨의 사이즈가 [32x32x3]이고, receptive field가 5x5라고 하자. 그러면 CONV layer의 각 뉴런은 인풋 볼륨의 [5x5x3] region에 대한 weight를 가진다. 위에서 언급했던것처럼, depth는 3으로 인풋의 뎁쓰와 동일하다.

- - -  
![conv layer](http://cs231n.github.io/assets/cnn/depthcol.jpeg)

**왼쪽**: [32x32x3]의 CIFAR-10 이미지 인풋 볼륨이 빨간색으로 표현되어 있다. 여기서 local region(but full depth)만 첫번째 CONV layer로 연결된다. 이 때, 여러개의 뉴런을 확인할 수 있는데(위 예제에선 5개), 이 뉴런들은 depth에 따라 전부 같은 region들을 가리킨다. 여기에 대해 자세한 건 아래에서 다룬다. 
**오른쪽**: 뉴런에서의 계산은 기존의 NN과 동일하다. input *x*와 weight *w*가 dot product로 곱해지고, 이 값들을 다 더해 bias *b*를 더한 뒤 activation function *f*에 넣어 최종적으로 output을 산출한다. 
> non-linearity는 activation function으로부터 얻어진다. *wx*는 linear이고, linear들의 합 또한 linear이다. 즉 activation function은 nonlinear를 사용해야 하며 이것이 NN에 non-linearity를 부여한다. http://stackoverflow.com/questions/9782071/why-must-a-nonlinear-activation-function-be-used-in-a-backpropagation-neural-net 참고.  
- - -  

**Spatial arrangement.** CONV layer의 input volume에 대해 지금까지 살펴봤는데, 이제 output volume에 대해 살펴보도록 하자. 세가지 하이퍼파라메터가 output volume의 사이즈를 결정한다: **depth, stride** 그리고 **zero-padding**:

1. **depth:** 아웃풋 볼륨의 뎁쓰는 우리가 선택할 수 있는 하이퍼파라메터다. 이것은 CONV layer에서 같은 region의 input volume연결되는 뉴런의 수를 결정한다. 이는 regular NN에서 히든 레이어의 여러 뉴런들이 같은 input을 받는것과 유사하다. 앞으로 살펴보겠지만, 이 뉴런들이 같은 region의 input을 받더라도 전부 다 다른 특성들을 학습한다. 예를 들어, 첫번째 CONV layer가 raw image를 input으로 받는다고 할 때, depth dimension에 따른 각각의 뉴런들이 다양한 방향의 엣지(various oriented edged) 또는 색 덩어리(blobs of color) 등의 특성에 의해 활성화된다. 앞으로 우리는 같은 region을 받는 뉴런들을 **depth column**이라고 할 것이다. (depth column이라고 하면 이 뉴런들을 통째로 지칭하는 것 같은 느낌이 드는데 하나하나도 depth column이라고 하는 듯 하다)
2. **stride:** stride 또한 명시적으로 지정해줘야 한다. stride가 1이면, 새로운 depth column을 할당할 때마다 1유닛씩만 띄어서 계산한다(아래 예제를 참고하자). 이는 column간의 receptive fields가 너무 겹치게 되고, 커다란 아웃풋 볼륨을 생성한다. 반대로 높은 stride를 사용한다면, receptive fields가 적게 겹치게 되고 가로세로(spatial)가 더 작은 아웃풋 볼륨을 생성한다.
3. **zero-padding**: 앞으로 살펴보겠지만, 인풋 볼륨의 경계(border)에 zero padding을 해 주는 것이 편리할 수 있다. 이 때 이 **zero-padding**의 사이즈가 하이퍼파라메터가 된다. 제로 패딩의 장점 중 하나는 아웃풋 볼륨의 spatial size를 컨트롤 할 수 있게 해 준다는 점인데, 때때로 인풋 볼륨의 spatial size를 유지해야 하는 경우가 있다.

인풋 볼륨의 사이즈($W$)로 아웃풋 볼륨의 spatial size를 계산할 수 있는데, 이 때 CONV Layer의 receptive field size ($F$), 그것들이 적용되는 stride ($S$), 그리고 경계에 적용되는 제로 패딩 ($P$)을 사용한다. 얼마나 많은 뉴런들이 "fit"한지 $(W-F+2P)/S+1$의 공식을 통해 계산할 ㄹ수 있다. 만약 이 결과가 정수가 아니라면, stride값이 잘못되어 인풋 볼륨에서의 필터의 적용이 깔끔하게 맞아 떨어지지 않는 것이다. 아래 예제가 도움이 될 것이다:

- - -
![formular example](http://cs231n.github.io/assets/cnn/stride.jpeg)

spatial arragnement를 보여주고 있다. 이 예제는 하나의 spatial dimension만을 가지고 있다 (x축). 각 뉴런의 receptive field size F는 3이고, 인풋 볼륨의 크기 W는 5, 그리고 제로 패딩 P는 1이다. 위 그림을 보면 인풋 볼륨 좌우로 0이 배치되어 있어(제로 패딩) 총 인풋 유닛이 7개인 것을 볼 수 있다(인풋 볼륨 5 + 제로 패딩 2). 
**왼쪽**: stride S = 1인 경우로, 한 칸씩 띄어가며 필터를 적용한다. 아웃풋 사이즈는 (5 - 3 + 2)/1+1 = 5.
**오른쪽**: stride S = 2인 경우. 아웃풋 사이즈는 (5 - 3 + 2)/2+1 = 3이 된다. 그림에서 직관적으로도 알 수 있고 공식으로도 알 수 있듯이 stride S = 3은 fit하지 않기 때문에 사용할 수 없다! 공식으로 계산해 보면 (5 - 3 + 2) = 4가 3으로 나누어 떨어지지 않는다.
위 예제에서 뉴런의 weight는 [1, 0, -1]이며 bias는 0이다. 이 weight들은 모든 아웃풋 뉴런들끼리 공유된다(parameter sharing).  
- - -

*Use of zero-padding.* 위의 왼쪽 예제에서, 인풋 디멘션이 5이고 아웃풋 디멘션도 동일하게 5인 것을 주목하자. 이는 receptive fields가 3이고 제로 패딩이 1이기 때문이다. 만약 제로 패딩을 사용하지 않았다면, 아웃풋 볼륨의 spatial 디멘션은 3일 것이다. 일반적으로, stride $S=1$일 때 제로 패딩을 $P=(F-1)/2$로 설정하면 인풋 볼륨과 아웃풋 볼륨이 같은 spatial size를 갖는다. 이것이 제로 패딩을 설정하는 일반적인 방법이며 이에 대한 자세한 이유는 차후 ConvNet 아키텍처를 다룰 때 다시 설명할 것이다.

*Constraints on strides.* spatial arrangement에서의 hyperparameter들은 공통의 제약(mutual cosntraint)을 갖는다. 예를 들어, 인풋의 사이즈가 $W=10$이고, 제로 패딩을 쓰지 않고 $P=0$, 필터 사이즈가 $F=3$이라면 $(W-F+2P)/S+1=(10-3+0)/2+1=4.5$이기 때문에 stride $S=2$는 사용할 수 없다. 앞으로 살펴볼 ConvNet 아키텍처 섹션에서, 모든 디멘션들이 잘 "work out"하도록 ConvNets의 사이즈를 적절하게 잘 정하는 것은 정말 머리아픈 일이다. 그나마, zero-padding의 사용과 몇몇 디자인 가이드라인들이 조금 이를 쉽게 해준다.

*Real-world example.* ImageNet 챌린지에서 2012년도에 우승한 [Krizhevsky et al.](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) 아키텍처는 [227x227x3]사이즈의 이미지를 사용한다. 첫번째 Convolution layer에서, receptive field size $F=11$, stride $S=4$ 그리고 제로 패딩을 사용하지 않는다 $P=0$. (227 - 11)/4 + 1 = 55 이고, Conv layer가 $K=96$의 depth를 가지므로, Conv layer의 아웃풋 볼륨은 [55x55x96]이 된다. 각 55\*55\*96의 뉴런들은 인풋 볼륨의 [11x11x3]의 region과 연결된다. 그리고 앞서 이야기한것처럼, 96개의 각 depth column들은 전부 같은 [11x11x3]의 인풋 region에 연결되지만, 전부 다 다른 weight를 갖는다.

**Parameter Sharing.** Parameter sharing scheme은 Convolution Layers에서 파라메터의 수를 컨트롤하기 위해 사용한다. 위의 실제 예제(real-world example)에서, 55*55*96 = 290,400개의 뉴런들이 첫번째 Conv layer에 있고, 각각은 11*11*3 = 363개의 weights와 1개의 bias를 갖는다. 즉 ConvNet의 첫번째 레이어에만 290400 * 364 = 105,705,600개의 파라메터들이 존재하는 것이다. 명백하게, 너무 많다.

하나의 타당한(reasonable) 가정을 통해 이 파라메터의 수를 엄청나게 줄일 수 있다: 만약 어떤 작은 특성이 어떤 spatial position (x, y)를 계산하는 데에 유용하다면, 이것은 또한 다른 포지션 (x2, y2)를 계산하는 데에도 유용할 것이다. 다시 말해서, 볼륨을 depth로 조각내어 각 2차원 slice를 **depth slice**라고 한다면 ([55x55x96]의 볼륨은 각각 [55x55]의 크기인 96개의 depth slice로 구성된다) , 각 depth slice의 뉴런들이 같은 weights와 bias를 사용하도록 제한할 수 있다. 이러한 parameter sharing scheme을 사용하면, 첫번째 Conv Layer는 이제 오직 96개의 unique weights set을 가지므로(각 depth slice마다 하나씩), 총 파라메터의 수는 96*11*11*3 = 34,848 개의 unqiue weights 또는 34,944 (+96 biases)개이다. 대신에, 이제 각 depth slice의 55*55개의 뉴런들은 같은 파라메터를 사용한다. 실제 backpropagation에서, 볼륨의 모든 뉴런들은 각 weights에 따라 gradient를 계산해야 하는데, 이 gradient들은 각 슬라이스마다 한 번 씩만 업데이트하면 된다.

이와 같이 single depth slice의 모든 뉴런들이 전부 동일한 weight vector를 사용한다면, CONV layer의 각 depth slice에서의 forward pass는 뉴런의 weights와 인풋 볼륨의 **convolution**으로 계산할 수 있다(그러므로 이름이 Convolution Layer이다). 따라서, 이 input과 함께 convolve되는 weights를 **filter** 또는 **kernel**이라고 부른다. 이 convolution의 결과는 *activation map* (이 예제에서는 [55x55])으로 나타나며 각각의 필터에 따른 activation map들의 집합이 depth dimension만큼 쌓여(stacked) 아웃풋 볼륨을 생성한다(이 예제에서는 [55x55x96]).

- - -
![filters](http://cs231n.github.io/assets/cnn/weights.jpeg)

Krizhevsky et al. 에서 학습한 필터 예제들이다. 총 96개의 필터들로 각각 [11x11x3]의 사이즈를 가지며, 각각 하나의 depth slice에서 55*55 개의 뉴런들이 공유하는 필터다. 
parameter sharing assumption이 비교적 합리적이라는 것에 주목하자: 만약 수평 엣지(horizontal edge)가 이미지의 특정 부분에서 중요한 특성이라면, 직관적으로, 이는 이미지의 어느 다른 부분에서도 중요한 특성일 것이다 - translationally-invariant structure of image때문에. 
- - -

때로는 parameter sharing assumption이 성립하지 않을 수도 있다. 이는 특히 ConvNet의 인풋 이미지가 명확한 centered structure를 갖고 있을 때 그러하다. 예를 들어, 우리는 보통 이미지의 양 사이드가 완전히 다른 특성을 갖고 있기를 기대한다. 실례를 들자면 centered structure인 얼굴 이미지를 보자. 이 이미지에서는 아마 포지션별로 눈 특성, 머리 특성 등이 학습되길 기대할 것이다. 이런 경우에는 parameter sharing scheme을 좀 약하게(relax) 하는 것이 일반적이고, 대신에 간단하게 이 레이어를 **Locally-Connected Layer**라 부른다.

... Numpy example 생략 ...

**Summary.** Conv Layer:
* Accepts a volume of size: $W_1 \times H_1 \times D_1$
* Requires four hyperparameters:
	* Number of filters (convolutions) $K$
	* their spatial extent (= window size) $F$
	* the stride $S$
	* the amount of zero padding $P$
* Produces a volume of size $W_2 \times H_2 \times D_2$ where:
	* $W_2 = (W_1 - F + 2P)/S + 1$
	* $H_2 = (H_1 - F + 2P)/S + 1$
	* $D_2 = K$
* Parameter sharing 하에서, 필터당 $F \cdot F \cdot D_1$ 개의 weight와 하나의 bias를 갖는다. 따라서 총 $(F \cdot F \cdot D_1) \cdot K$ 개의 weight와 $K$ 개의 bias를 갖는다. 
	* 즉 parameter sharing 하에서 필터는 Depth 만큼의 깊이를 가진다.

**Convolution Demo.** 이 부분은 원문을 참고하자. 훌륭한 애니메이션을 제공한다.

#### Pooling Layer

...

## 참고
[An Interactive 3D Visualization of Fully-Connected and Convolutional Neural Networks](http://scs.ryerson.ca/~aharley/vis/)
